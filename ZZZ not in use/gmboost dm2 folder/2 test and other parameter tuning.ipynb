{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the best result we got was\n",
    "\n",
    "    \"loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\": {\n",
    "        \"accuracy\": 0.14258154406773607,\n",
    "        \"precision\": 0.1628068243471998,\n",
    "        \"recall\": 0.12970016045006125,\n",
    "        \"f1Score\": 0.14721974219604964\n",
    "    },\n",
    "    \"loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\": {\n",
    "        \"accuracy\": 0.14258154406773607,\n",
    "        \"precision\": 0.1628068243471998,\n",
    "        \"recall\": 0.12970016045006125,\n",
    "        \"f1Score\": 0.14721974219604964\n",
    "    },\n",
    "    \"loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\": {\n",
    "        \"accuracy\": 0.14258154406773607,\n",
    "        \"precision\": 0.1628068243471998,\n",
    "        \"recall\": 0.12970016045006125,\n",
    "        \"f1Score\": 0.14721974219604964\n",
    "    },\n",
    "    \"loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\": {\n",
    "        \"accuracy\": 0.14258154406773607,\n",
    "        \"precision\": 0.1628068243471998,\n",
    "        \"recall\": 0.12970016045006125,\n",
    "        \"f1Score\": 0.14721974219604964\n",
    "    },\n",
    "    \"loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\": {\n",
    "        \"accuracy\": 0.14258154406773607,\n",
    "        \"precision\": 0.1628068243471998,\n",
    "        \"recall\": 0.12970016045006125,\n",
    "        \"f1Score\": 0.14721974219604964\n",
    "    },\n",
    "    \"loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\": {\n",
    "        \"accuracy\": 0.14258154406773607,\n",
    "        \"precision\": 0.1628068243471998,\n",
    "        \"recall\": 0.12970016045006125,\n",
    "        \"f1Score\": 0.14721974219604964\n",
    "    },\n",
    "\n",
    "as we can notice going the best learning rate is always 0.1, the best mean samples split is also 2 and the best tolerance is still 0.01 as we observed in the training part \n",
    "\n",
    "instead using a higher number of esitmators improved the results, also the mid value (2) for the min samples leaf imrpoved it and the higher max depth improved the results too\n",
    "\n",
    "i decided than to use a even higher number of esitmators and a even higher number of max depth while keeping the other parameters as they came"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#previous parameters\n",
    "loss = \"log_loss\"\n",
    "learningRate = 0.1\n",
    "tolerance = 0.01 #noticed after that the other best tolerance was 0.001 but this should not change the results much\n",
    "minSamplesSplit = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "minSamplesLeaf = 2 #keeping the new best value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itrerating close values to both see if the difference is that big and also since using a higher number make the model take way longer\n",
    "n_esitimators = [50, 75, 100, 125]\n",
    "max_depths = [5, 10, 15] #same reasoning here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = r\"C:\\Users\\alex1\\Desktop\\DataMining2\\dataset\\tabular\\finalDataset.csv\"\n",
    "testPath = r\"C:\\Users\\alex1\\Desktop\\DataMining2\\dataset\\tabular\\test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf = pd.read_csv(trainPath)\n",
    "testDf = pd.read_csv(testPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = trainDf.drop(\"key\", axis=1)\n",
    "yTrain = trainDf[\"key\"]\n",
    "\n",
    "xTest = testDf.drop(\"key\", axis=1)\n",
    "yTest = testDf[\"key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = StandardScaler()\n",
    "xTrainScaled = scl.fit_transform(xTrain)\n",
    "xTestScaled = scl.fit_transform(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 2 max_depth: 10 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 2 max_depth: 15 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 10 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 15 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 100 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 100 min_samples_split: 2 min_samples_leaf: 2 max_depth: 10 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 100 min_samples_split: 2 min_samples_leaf: 2 max_depth: 15 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 125 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 125 min_samples_split: 2 min_samples_leaf: 2 max_depth: 10 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 125 min_samples_split: 2 min_samples_leaf: 2 max_depth: 15 tolerance: 0.01\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for n_esitimator in n_esitimators:\n",
    "    for max_depth in max_depths:\n",
    "        clf = GradientBoostingClassifier(loss=loss, learning_rate=learningRate, \n",
    "                            n_estimators=n_esitimator, min_samples_split=minSamplesSplit, \n",
    "                            min_samples_leaf=minSamplesLeaf, max_depth=max_depth, \n",
    "                            tol=tolerance, random_state=42)\n",
    "\n",
    "        key = \" \".join([\"loss:\", str(loss), \"learningRate:\", str(learningRate), \"n_estimator:\", str(n_esitimator), \n",
    "            \"min_samples_split:\", str(minSamplesSplit), \"min_samples_leaf:\", str(minSamplesLeaf), \"max_depth:\", str(max_depth),\n",
    "            \"tolerance:\", str(tolerance)])\n",
    "        \n",
    "        print (key)\n",
    "        \n",
    "        clf.fit(xTrainScaled, yTrain)\n",
    "        y_pred = clf.predict(xTestScaled)\n",
    "        \n",
    "        \n",
    "        accuracy = accuracy_score(yTest, y_pred)\n",
    "        # Precision\n",
    "        precision = precision_score(yTest, y_pred, average='weighted', zero_division=0)\n",
    "        # Recall\n",
    "        recall = recall_score(yTest, y_pred, average='macro', zero_division=0)\n",
    "        # F1 Score\n",
    "        f1 = f1_score(yTest, y_pred, average='weighted')\n",
    "        \n",
    "        results[key] = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1Score\": f1,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (r\"C:\\Users\\alex1\\Desktop\\DataMining2\\src\\Classification\\Tabular\\Gradient Boosting Machines\\results on test dataset with parameter tuning 1.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dictionary keys based on the chosen performance metric\n",
    "sorted_keys = sorted(results.keys(), key=lambda x: (results[x][\"accuracy\"], results[x][\"precision\"], results[x][\"recall\"], results[x][\"f1Score\"]), reverse=True)\n",
    "\n",
    "# Create a new dictionary with the ordered keys\n",
    "ordered_dict = {key: results[key] for key in sorted_keys}\n",
    "\n",
    "# Save the ordered dictionary to a JSON file\n",
    "with open (r\"C:\\Users\\alex1\\Desktop\\DataMining2\\src\\Classification\\Tabular\\Gradient Boosting Machines\\results on test dataset with parameter tuning 1 sorted.json\", \"w\") as f:    \n",
    "    json.dump(ordered_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

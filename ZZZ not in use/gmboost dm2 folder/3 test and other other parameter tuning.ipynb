{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the best result we got were\n",
    "\n",
    "    \"loss: log_loss learningRate: 0.1 n_estimator: 125 min_samples_split: 2 min_samples_leaf: 2 max_depth: 15 tolerance: 0.01\": {\n",
    "    \"accuracy\": 0.18273730842056377,\n",
    "    \"precision\": 0.20007378893100994,\n",
    "    \"recall\": 0.16731702281785207,\n",
    "    \"f1Score\": 0.1887470571283209\n",
    "    },\n",
    "    \"loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 15 tolerance: 0.01\": {\n",
    "        \"accuracy\": 0.1807545282412204,\n",
    "        \"precision\": 0.19879526361903635,\n",
    "        \"recall\": 0.16600504488063714,\n",
    "        \"f1Score\": 0.18724790849276787\n",
    "    },\n",
    "    \"loss: log_loss learningRate: 0.1 n_estimator: 100 min_samples_split: 2 min_samples_leaf: 2 max_depth: 15 tolerance: 0.01\": {\n",
    "        \"accuracy\": 0.1805580365117359,\n",
    "        \"precision\": 0.19893474117598844,\n",
    "        \"recall\": 0.1660938350103445,\n",
    "        \"f1Score\": 0.18687368730172618\n",
    "    },\n",
    "    \"loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 2 max_depth: 15 tolerance: 0.01\": {\n",
    "        \"accuracy\": 0.1796113036333107,\n",
    "        \"precision\": 0.1968913303254834,\n",
    "        \"recall\": 0.1644037931994303,\n",
    "        \"f1Score\": 0.18608577327169679\n",
    "    },\n",
    "\n",
    "as we can notice the number of estimators did not really make a change, instead what made the biggest change was the max_depth\n",
    "\n",
    "since those are the results we setted the number of esitamtors to 75 which seems a good compromise between the number we used and the time it takes to train the model and expanded the search on the max_depth parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#previous parameters\n",
    "loss = \"log_loss\"\n",
    "learningRate = 0.1\n",
    "tolerance = 0.01 \n",
    "minSamplesSplit = 2 \n",
    "minSamplesLeaf = 2 \n",
    "n_esitamtor = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = [15, 30, 50, 70, 100] #same reasoning here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = r\"C:\\Users\\alex1\\Desktop\\DataMining2\\dataset\\tabular\\finalDataset.csv\"\n",
    "testPath = r\"C:\\Users\\alex1\\Desktop\\DataMining2\\dataset\\tabular\\test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf = pd.read_csv(trainPath)\n",
    "testDf = pd.read_csv(testPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = trainDf.drop(\"key\", axis=1)\n",
    "yTrain = trainDf[\"key\"]\n",
    "\n",
    "xTest = testDf.drop(\"key\", axis=1)\n",
    "yTest = testDf[\"key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = StandardScaler()\n",
    "xTrainScaled = scl.fit_transform(xTrain)\n",
    "xTestScaled = scl.fit_transform(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 15 tolerance: 0.01\n",
      "{'accuracy': 0.1807545282412204, 'precision': 0.19879526361903635, 'recall': 0.16600504488063714, 'f1Score': 0.18724790849276787}\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 30 tolerance: 0.01\n",
      "{'accuracy': 0.18066521381872744, 'precision': 0.20160728952936094, 'recall': 0.1677636383276256, 'f1Score': 0.18731338771533512}\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 50 tolerance: 0.01\n",
      "{'accuracy': 0.17837876460290808, 'precision': 0.20115184700516628, 'recall': 0.16588888701002538, 'f1Score': 0.18643429874709352}\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 70 tolerance: 0.01\n",
      "{'accuracy': 0.17773570076095888, 'precision': 0.201219018400372, 'recall': 0.16607692234072666, 'f1Score': 0.18577541711603995}\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 100 tolerance: 0.01\n",
      "{'accuracy': 0.17637812153906612, 'precision': 0.20025925895535834, 'recall': 0.16476355191125028, 'f1Score': 0.18459611335844506}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for max_depth in max_depths:\n",
    "    clf = GradientBoostingClassifier(loss=loss, learning_rate=learningRate, \n",
    "                        n_estimators=n_esitamtor, min_samples_split=minSamplesSplit, \n",
    "                        min_samples_leaf=minSamplesLeaf, max_depth=max_depth, \n",
    "                        tol=tolerance, random_state=42)\n",
    "\n",
    "    key = \" \".join([\"loss:\", str(loss), \"learningRate:\", str(learningRate), \"n_estimator:\", str(n_esitamtor), \n",
    "        \"min_samples_split:\", str(minSamplesSplit), \"min_samples_leaf:\", str(minSamplesLeaf), \"max_depth:\", str(max_depth),\n",
    "        \"tolerance:\", str(tolerance)])\n",
    "    \n",
    "    print (key)\n",
    "    \n",
    "    clf.fit(xTrainScaled, yTrain)\n",
    "    y_pred = clf.predict(xTestScaled)\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(yTest, y_pred)\n",
    "    # Precision\n",
    "    precision = precision_score(yTest, y_pred, average='weighted', zero_division=0)\n",
    "    # Recall\n",
    "    recall = recall_score(yTest, y_pred, average='macro', zero_division=0)\n",
    "    # F1 Score\n",
    "    f1 = f1_score(yTest, y_pred, average='weighted')\n",
    "    \n",
    "    results[key] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1Score\": f1,\n",
    "        }\n",
    "    print (results [key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 15 tolerance: 0.01': {'accuracy': 0.1807545282412204,\n",
       "  'precision': 0.19879526361903635,\n",
       "  'recall': 0.16600504488063714,\n",
       "  'f1Score': 0.18724790849276787},\n",
       " 'loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 30 tolerance: 0.01': {'accuracy': 0.18066521381872744,\n",
       "  'precision': 0.20160728952936094,\n",
       "  'recall': 0.1677636383276256,\n",
       "  'f1Score': 0.18731338771533512},\n",
       " 'loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 50 tolerance: 0.01': {'accuracy': 0.17837876460290808,\n",
       "  'precision': 0.20115184700516628,\n",
       "  'recall': 0.16588888701002538,\n",
       "  'f1Score': 0.18643429874709352},\n",
       " 'loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 70 tolerance: 0.01': {'accuracy': 0.17773570076095888,\n",
       "  'precision': 0.201219018400372,\n",
       "  'recall': 0.16607692234072666,\n",
       "  'f1Score': 0.18577541711603995},\n",
       " 'loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 100 tolerance: 0.01': {'accuracy': 0.17637812153906612,\n",
       "  'precision': 0.20025925895535834,\n",
       "  'recall': 0.16476355191125028,\n",
       "  'f1Score': 0.18459611335844506}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (r\"C:\\Users\\alex1\\Desktop\\DataMining2\\src\\Classification\\Tabular\\Gradient Boosting Machines\\results on test dataset with parameter tuning 2.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dictionary keys based on the chosen performance metric\n",
    "sorted_keys = sorted(results.keys(), key=lambda x: (results[x][\"accuracy\"], results[x][\"precision\"], results[x][\"recall\"], results[x][\"f1Score\"]), reverse=True)\n",
    "\n",
    "# Create a new dictionary with the ordered keys\n",
    "ordered_dict = {key: results[key] for key in sorted_keys}\n",
    "\n",
    "# Save the ordered dictionary to a JSON file\n",
    "with open (r\"C:\\Users\\alex1\\Desktop\\DataMining2\\src\\Classification\\Tabular\\Gradient Boosting Machines\\results on test dataset with parameter tuning 2 sorted.json\", \"w\") as f:    \n",
    "    json.dump(ordered_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the best results were achived using the combination of the following parameters:\n",
    "\n",
    "holdout 0.01 {'loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001': {'accuracy': 0.14706592313539252, 'precision': 0.14801640190488102, 'recall': 0.14723335299628126, 'f1Score': 0.14060405534462095}}\n",
    "\n",
    "holdout 0.25 {'loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001': {'accuracy': 0.14513028207075102, 'precision': 0.14393154878627729, 'recall': 0.14546596893938116, 'f1Score': 0.1391188574071227}}\n",
    "\n",
    "holdout 0.5 {'loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001': {'accuracy': 0.14090850120431192, 'precision': 0.13954088568757558, 'recall': 0.14133017932994482, 'f1Score': 0.1333028517791039}}\n",
    "\n",
    "holdout 0.75 {'loss: log_loss learningRate: 0.1 n_estimator: 25 min_samples_split: 2 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001': {'accuracy': 0.1309211364693819, 'precision': 0.13103099684433572, 'recall': 0.13146097823017008, 'f1Score': 0.12208886451904581}}\n",
    "\n",
    "holdout 0.9 {'loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 1 max_depth: 2 tolerance: 0.001': {'accuracy': 0.12722866780339537, 'precision': 0.12977156730252887, 'recall': 0.1281885037841295, 'f1Score': 0.12224727277625198}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the results dont change that much between the various holdouts i decided to take the parameters that are the most frequent and their arounds and use them to train the final model and to use the full train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "#the parameters here are, at first, the original best value, and then the new values to try\n",
    "loss = \"log_loss\" \n",
    "learningRates = [0.2, 0.1, 0.05] #0.1 is better for bigger holdouts, adding 0.05 so maybe the results will improve, since i'm using the full dataset \n",
    "n_estimators = [50, 75] #added 75 since 50 was the most frequent, maybe the results go up \n",
    "min_samples_splits = [2, 3, 4] # 3 out of 5 are with 2, decided to add the middle value\n",
    "min_samples_leafs = [3, 1, 2] # same reasoning as before\n",
    "max_depths = [3,  5] # most results are with 3, maybe going higher will improve it \n",
    "tolerances = [0.01, 0.001] # all results are with 0.001, the initial parameters where 0.001 and 0.0001, going higher might help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = r\"C:\\Users\\alex1\\Desktop\\DataMining2\\dataset\\tabular\\finalDataset.csv\"\n",
    "testPath = r\"C:\\Users\\alex1\\Desktop\\DataMining2\\dataset\\tabular\\test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf = pd.read_csv(trainPath)\n",
    "testDf = pd.read_csv(testPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mode', 'energy', 'acousticness', 'loudness', 'danceability', 'valence',\n",
       "       'speechiness', 'n_beats', 'time_signature', 'key'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mode', 'energy', 'acousticness', 'loudness', 'danceability', 'valence',\n",
       "       'speechiness', 'n_beats', 'time_signature', 'key'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = trainDf.drop(\"key\", axis=1)\n",
    "yTrain = trainDf[\"key\"]\n",
    "\n",
    "xTest = testDf.drop(\"key\", axis=1)\n",
    "yTest = testDf[\"key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = StandardScaler()\n",
    "xTrainScaled = scl.fit_transform(xTrain)\n",
    "xTestScaled = scl.fit_transform(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32844, 9), (55982, 9))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrainScaled.shape, xTestScaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.2 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.1 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 3 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 50 min_samples_split: 4 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 2 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 3 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 3 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 3 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 3 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 3 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 1 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 1 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 1 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 1 max_depth: 5 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 2 max_depth: 3 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 2 max_depth: 3 tolerance: 0.001\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 2 max_depth: 5 tolerance: 0.01\n",
      "loss: log_loss learningRate: 0.05 n_estimator: 75 min_samples_split: 4 min_samples_leaf: 2 max_depth: 5 tolerance: 0.001\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for learningRate in learningRates:\n",
    "    for n_estimator in n_estimators:\n",
    "        for min_samples_split in min_samples_splits:\n",
    "            for min_samples_leaf in min_samples_leafs:\n",
    "                for max_depth in max_depths:\n",
    "                    for tolerance in tolerances:              \n",
    "                        clf = GradientBoostingClassifier(loss=loss, learning_rate=learningRate, \n",
    "                                         n_estimators=n_estimator, min_samples_split=min_samples_split, \n",
    "                                         min_samples_leaf=min_samples_leaf, max_depth=max_depth, \n",
    "                                         tol=tolerance, random_state=42)\n",
    "\n",
    "                        key = \" \".join([\"loss:\", str(loss), \"learningRate:\", str(learningRate), \"n_estimator:\", str(n_estimator), \n",
    "                            \"min_samples_split:\", str(min_samples_split), \"min_samples_leaf:\", str(min_samples_leaf), \"max_depth:\", str(max_depth),\n",
    "                            \"tolerance:\", str(tolerance)])\n",
    "                        \n",
    "                        print (key)\n",
    "                        \n",
    "                        clf.fit(xTrainScaled, yTrain)\n",
    "                        y_pred = clf.predict(xTestScaled)\n",
    "                        \n",
    "                        \n",
    "                        accuracy = accuracy_score(yTest, y_pred)\n",
    "                        # Precision\n",
    "                        precision = precision_score(yTest, y_pred, average='weighted', zero_division=0)\n",
    "                        # Recall\n",
    "                        recall = recall_score(yTest, y_pred, average='macro', zero_division=0)\n",
    "                        # F1 Score\n",
    "                        f1 = f1_score(yTest, y_pred, average='weighted')\n",
    "                        \n",
    "                        results[key] = {\n",
    "                                \"accuracy\": accuracy,\n",
    "                                \"precision\": precision,\n",
    "                                \"recall\": recall,\n",
    "                                \"f1Score\": f1,\n",
    "                            }\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (r\"C:\\Users\\alex1\\Desktop\\DataMining2\\src\\Classification\\Tabular\\Gradient Boosting Machines\\results on test dataset.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dictionary keys based on the chosen performance metric\n",
    "sorted_keys = sorted(results.keys(), key=lambda x: (results[x][\"accuracy\"], results[x][\"precision\"], results[x][\"recall\"], results[x][\"f1Score\"]), reverse=True)\n",
    "\n",
    "# Create a new dictionary with the ordered keys\n",
    "ordered_dict = {key: results[key] for key in sorted_keys}\n",
    "\n",
    "# Save the ordered dictionary to a JSON file\n",
    "with open (r\"C:\\Users\\alex1\\Desktop\\DataMining2\\src\\Classification\\Tabular\\Gradient Boosting Machines\\results on test dataset sorted.json\", \"w\") as f:    \n",
    "    json.dump(ordered_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
